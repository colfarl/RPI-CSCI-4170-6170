{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Projects in Machine Learning and AI (RPI Spring 2025)\n",
        "# Homework 6\n",
        "\n",
        "## Outline of This Notebook\n",
        "1. **Part 1: Transformers** (Task 1)\n",
        "   - Dataset Selection, Split\n",
        "   - Training Script for BART Summarization\n",
        "   - Fine-Tuning + Results (BLEU, ROUGE)\n",
        "   - Analysis of Hyperparameters & Model Choice\n",
        "\n",
        "2. **Part 2: Reinforcement Learning**\n",
        "   - **Task 2**: Real-World MDP Formulation\n",
        "   - **Task 3**: RL Application in Selected Domain + Open-Source Project\n",
        "\n",
        "3. **Part 3: Recommender Systems** (Task 5)\n",
        "   - MovieLens 100k Dataset\n",
        "   - Data Cleaning, EDA, User-Item Matrix\n",
        "   - Implementation of Two Collaborative Filtering Methods\n",
        "   - Comparison on Two Evaluation Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Transformers\n",
        "\n",
        "In **Task 1**, we will use the [Facebook BART model](https://huggingface.co/docs/transformers/model_doc/bart) to perform **text summarization** on the CNN/DailyMail news dataset, which is a common text-summarization benchmark.\n",
        "\n",
        "Below are the steps we will follow:\n",
        "1. **Select & Describe Dataset** + **Train/Test Split (90-10)** (though the official dataset is already split into train/validation/test, we'll demonstrate a splitting workflow if needed).\n",
        "2. **Load BART Model** from Hugging Face Transformers + **Training Script**.\n",
        "3. **Fine-tune** Pre-trained Model + **Report BLEU and ROUGE**.\n",
        "4. **Analysis** of Results, Hyperparameters & Choice of LLM.\n",
        "\n",
        "### Dataset Choice\n",
        "\n",
        "The **CNN/DailyMail** dataset consists of news articles and corresponding highlights (summaries). We have already downloaded the dataset in our `data/cnn_dailymail` folder, containing three CSV files:\n",
        "\n",
        "- `train.csv` (~285,000 rows)\n",
        "- `validation.csv` (~13,300 rows)\n",
        "- `test.csv` (~11,500 rows)\n",
        "\n",
        "Each CSV has two main columns (commonly `article` and `highlights` or `summary`) that we will use for summarization. This notebook provides a structure for training and evaluating a summarization model on these files.\n",
        "\n",
        "## 1. Dataset Description & Potential Splitting\n",
        "\n",
        "The **CNN/DailyMail** dataset pairs a long news article with a concise summary (often bullet points called *highlights*). For each row:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "In many workflows, you could combine all data and manually split 90/10. However, since the dataset already comes partitioned, we can either:\n",
        "- Use the official `train.csv`, `validation.csv`, `test.csv` splits, **or**\n",
        "- Combine them if we wish and re-split. \n",
        "\n",
        "Below, we will simply load the provided CSV files for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution_count": 1
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set size: 287113\n",
            "Validation set size: 13368\n",
            "Test set size: 11490\n",
            "\n",
            "Sample rows from the training set:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
              "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
              "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
              "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
              "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
              "      <td>A drunk driver who killed a young woman in a h...</td>\n",
              "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id  \\\n",
              "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
              "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
              "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
              "\n",
              "                                             article  \\\n",
              "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
              "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
              "2  A drunk driver who killed a young woman in a h...   \n",
              "\n",
              "                                          highlights  \n",
              "0  Bishop John Folda, of North Dakota, is taking ...  \n",
              "1  Criminal complaint: Cop used his role to help ...  \n",
              "2  Craig Eccleston-Todd, 27, had drunk at least t...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the train, validation, and test sets from the local folder\n",
        "train_df = pd.read_csv(\"data/cnn_dailymail/train.csv\")\n",
        "valid_df = pd.read_csv(\"data/cnn_dailymail/validation.csv\")\n",
        "test_df = pd.read_csv(\"data/cnn_dailymail/test.csv\")\n",
        "\n",
        "# Print basic stats and a small sample\n",
        "print(f\"Train set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(valid_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "# Inspect the first few rows of training data\n",
        "print(\"\\nSample rows from the training set:\")\n",
        "display(train_df.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load BART Model & Define Training Script\n",
        "\n",
        "We'll now load the [Hugging Face Transformers library](https://github.com/huggingface/transformers) and initialize the `facebook/bart-large-cnn` model, which is specialized for summarization.\n",
        "\n",
        "### Outline\n",
        "1. **Load Pre-trained Tokenizer and Model**:\n",
        "   ```python\n",
        "   from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "   tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "   model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "2. Tokenize & Encode the articles and summaries.\n",
        "\n",
        "3. Prepare a PyTorch Dataset & DataLoader for training and testing.\n",
        "\n",
        "4. Train using standard seq2seq objectives (e.g., cross-entropy).\n",
        "\n",
        "5. Evaluate on the test set.\n",
        "\n",
        "Below is a training loop for a limited number of epochs (to illustrate code structure). In practice, we would use the Trainer API from transformers or a more robust custom loop with more epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model loaded...\n",
            "dataset loaded...\n",
            "training started...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/colinfarley/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch 1/500 complete\n",
            "batch 2/500 complete\n",
            "batch 3/500 complete\n",
            "batch 4/500 complete\n",
            "batch 5/500 complete\n",
            "batch 6/500 complete\n",
            "batch 7/500 complete\n",
            "batch 8/500 complete\n",
            "batch 9/500 complete\n",
            "batch 10/500 complete\n",
            "batch 11/500 complete\n",
            "batch 12/500 complete\n",
            "batch 13/500 complete\n",
            "batch 14/500 complete\n",
            "batch 15/500 complete\n",
            "batch 16/500 complete\n",
            "batch 17/500 complete\n",
            "batch 18/500 complete\n",
            "batch 19/500 complete\n",
            "batch 20/500 complete\n",
            "batch 21/500 complete\n",
            "batch 22/500 complete\n",
            "batch 23/500 complete\n",
            "batch 24/500 complete\n",
            "batch 25/500 complete\n",
            "batch 26/500 complete\n",
            "batch 27/500 complete\n",
            "batch 28/500 complete\n",
            "batch 29/500 complete\n",
            "batch 30/500 complete\n",
            "batch 31/500 complete\n",
            "batch 32/500 complete\n",
            "batch 33/500 complete\n",
            "batch 34/500 complete\n",
            "batch 35/500 complete\n",
            "batch 36/500 complete\n",
            "batch 37/500 complete\n",
            "batch 38/500 complete\n",
            "batch 39/500 complete\n",
            "batch 40/500 complete\n",
            "batch 41/500 complete\n",
            "batch 42/500 complete\n",
            "batch 43/500 complete\n",
            "batch 44/500 complete\n",
            "batch 45/500 complete\n",
            "batch 46/500 complete\n",
            "batch 47/500 complete\n",
            "batch 48/500 complete\n",
            "batch 49/500 complete\n",
            "batch 50/500 complete\n",
            "batch 51/500 complete\n",
            "batch 52/500 complete\n",
            "batch 53/500 complete\n",
            "batch 54/500 complete\n",
            "batch 55/500 complete\n",
            "batch 56/500 complete\n",
            "batch 57/500 complete\n",
            "batch 58/500 complete\n",
            "batch 59/500 complete\n",
            "batch 60/500 complete\n",
            "batch 61/500 complete\n",
            "batch 62/500 complete\n",
            "batch 63/500 complete\n",
            "batch 64/500 complete\n",
            "batch 65/500 complete\n",
            "batch 66/500 complete\n",
            "batch 67/500 complete\n",
            "batch 68/500 complete\n",
            "batch 69/500 complete\n",
            "batch 70/500 complete\n",
            "batch 71/500 complete\n",
            "batch 72/500 complete\n",
            "batch 73/500 complete\n",
            "batch 74/500 complete\n",
            "batch 75/500 complete\n",
            "batch 76/500 complete\n",
            "batch 77/500 complete\n",
            "batch 78/500 complete\n",
            "batch 79/500 complete\n",
            "batch 80/500 complete\n",
            "batch 81/500 complete\n",
            "batch 82/500 complete\n",
            "batch 83/500 complete\n",
            "batch 84/500 complete\n",
            "batch 85/500 complete\n",
            "batch 86/500 complete\n",
            "batch 87/500 complete\n",
            "batch 88/500 complete\n",
            "batch 89/500 complete\n",
            "batch 90/500 complete\n",
            "batch 91/500 complete\n",
            "batch 92/500 complete\n",
            "batch 93/500 complete\n",
            "batch 94/500 complete\n",
            "batch 95/500 complete\n",
            "batch 96/500 complete\n",
            "batch 97/500 complete\n",
            "batch 98/500 complete\n",
            "batch 99/500 complete\n",
            "batch 100/500 complete\n",
            "batch 101/500 complete\n",
            "batch 102/500 complete\n",
            "batch 103/500 complete\n",
            "batch 104/500 complete\n",
            "batch 105/500 complete\n",
            "batch 106/500 complete\n",
            "batch 107/500 complete\n",
            "batch 108/500 complete\n",
            "batch 109/500 complete\n",
            "batch 110/500 complete\n",
            "batch 111/500 complete\n",
            "batch 112/500 complete\n",
            "batch 113/500 complete\n",
            "batch 114/500 complete\n",
            "batch 115/500 complete\n",
            "batch 116/500 complete\n",
            "batch 117/500 complete\n",
            "batch 118/500 complete\n",
            "batch 119/500 complete\n",
            "batch 120/500 complete\n",
            "batch 121/500 complete\n",
            "batch 122/500 complete\n",
            "batch 123/500 complete\n",
            "batch 124/500 complete\n",
            "batch 125/500 complete\n",
            "batch 126/500 complete\n",
            "batch 127/500 complete\n",
            "batch 128/500 complete\n",
            "batch 129/500 complete\n",
            "batch 130/500 complete\n",
            "batch 131/500 complete\n",
            "batch 132/500 complete\n",
            "batch 133/500 complete\n",
            "batch 134/500 complete\n",
            "batch 135/500 complete\n",
            "batch 136/500 complete\n",
            "batch 137/500 complete\n",
            "batch 138/500 complete\n",
            "batch 139/500 complete\n",
            "batch 140/500 complete\n",
            "batch 141/500 complete\n",
            "batch 142/500 complete\n",
            "batch 143/500 complete\n",
            "batch 144/500 complete\n",
            "batch 145/500 complete\n",
            "batch 146/500 complete\n",
            "batch 147/500 complete\n",
            "batch 148/500 complete\n",
            "batch 149/500 complete\n",
            "batch 150/500 complete\n",
            "batch 151/500 complete\n",
            "batch 152/500 complete\n",
            "batch 153/500 complete\n",
            "batch 154/500 complete\n",
            "batch 155/500 complete\n",
            "batch 156/500 complete\n",
            "batch 157/500 complete\n",
            "batch 158/500 complete\n",
            "batch 159/500 complete\n",
            "batch 160/500 complete\n",
            "batch 161/500 complete\n",
            "batch 162/500 complete\n",
            "batch 163/500 complete\n",
            "batch 164/500 complete\n",
            "batch 165/500 complete\n",
            "batch 166/500 complete\n",
            "batch 167/500 complete\n",
            "batch 168/500 complete\n",
            "batch 169/500 complete\n",
            "batch 170/500 complete\n",
            "batch 171/500 complete\n",
            "batch 172/500 complete\n",
            "batch 173/500 complete\n",
            "batch 174/500 complete\n",
            "batch 175/500 complete\n",
            "batch 176/500 complete\n",
            "batch 177/500 complete\n",
            "batch 178/500 complete\n",
            "batch 179/500 complete\n",
            "batch 180/500 complete\n",
            "batch 181/500 complete\n",
            "batch 182/500 complete\n",
            "batch 183/500 complete\n",
            "batch 184/500 complete\n",
            "batch 185/500 complete\n",
            "batch 186/500 complete\n",
            "batch 187/500 complete\n",
            "batch 188/500 complete\n",
            "batch 189/500 complete\n",
            "batch 190/500 complete\n",
            "batch 191/500 complete\n",
            "batch 192/500 complete\n",
            "batch 193/500 complete\n",
            "batch 194/500 complete\n",
            "batch 195/500 complete\n",
            "batch 196/500 complete\n",
            "batch 197/500 complete\n",
            "batch 198/500 complete\n",
            "batch 199/500 complete\n",
            "batch 200/500 complete\n",
            "batch 201/500 complete\n",
            "batch 202/500 complete\n",
            "batch 203/500 complete\n",
            "batch 204/500 complete\n",
            "batch 205/500 complete\n",
            "batch 206/500 complete\n",
            "batch 207/500 complete\n",
            "batch 208/500 complete\n",
            "batch 209/500 complete\n",
            "batch 210/500 complete\n",
            "batch 211/500 complete\n",
            "batch 212/500 complete\n",
            "batch 213/500 complete\n",
            "batch 214/500 complete\n",
            "batch 215/500 complete\n",
            "batch 216/500 complete\n",
            "batch 217/500 complete\n",
            "batch 218/500 complete\n",
            "batch 219/500 complete\n",
            "batch 220/500 complete\n",
            "batch 221/500 complete\n",
            "batch 222/500 complete\n",
            "batch 223/500 complete\n",
            "batch 224/500 complete\n",
            "batch 225/500 complete\n",
            "batch 226/500 complete\n",
            "batch 227/500 complete\n",
            "batch 228/500 complete\n",
            "batch 229/500 complete\n",
            "batch 230/500 complete\n",
            "batch 231/500 complete\n",
            "batch 232/500 complete\n",
            "batch 233/500 complete\n",
            "batch 234/500 complete\n",
            "batch 235/500 complete\n",
            "batch 236/500 complete\n",
            "batch 237/500 complete\n",
            "batch 238/500 complete\n",
            "batch 239/500 complete\n",
            "batch 240/500 complete\n",
            "batch 241/500 complete\n",
            "batch 242/500 complete\n",
            "batch 243/500 complete\n",
            "batch 244/500 complete\n",
            "batch 245/500 complete\n",
            "batch 246/500 complete\n",
            "batch 247/500 complete\n",
            "batch 248/500 complete\n",
            "batch 249/500 complete\n",
            "batch 250/500 complete\n",
            "batch 251/500 complete\n",
            "batch 252/500 complete\n",
            "batch 253/500 complete\n",
            "batch 254/500 complete\n",
            "batch 255/500 complete\n",
            "batch 256/500 complete\n",
            "batch 257/500 complete\n",
            "batch 258/500 complete\n",
            "batch 259/500 complete\n",
            "batch 260/500 complete\n",
            "batch 261/500 complete\n",
            "batch 262/500 complete\n",
            "batch 263/500 complete\n",
            "batch 264/500 complete\n",
            "batch 265/500 complete\n",
            "batch 266/500 complete\n",
            "batch 267/500 complete\n",
            "batch 268/500 complete\n",
            "batch 269/500 complete\n",
            "batch 270/500 complete\n",
            "batch 271/500 complete\n",
            "batch 272/500 complete\n",
            "batch 273/500 complete\n",
            "batch 274/500 complete\n",
            "batch 275/500 complete\n",
            "batch 276/500 complete\n",
            "batch 277/500 complete\n",
            "batch 278/500 complete\n",
            "batch 279/500 complete\n",
            "batch 280/500 complete\n",
            "batch 281/500 complete\n",
            "batch 282/500 complete\n",
            "batch 283/500 complete\n",
            "batch 284/500 complete\n",
            "batch 285/500 complete\n",
            "batch 286/500 complete\n",
            "batch 287/500 complete\n",
            "batch 288/500 complete\n",
            "batch 289/500 complete\n",
            "batch 290/500 complete\n",
            "batch 291/500 complete\n",
            "batch 292/500 complete\n",
            "batch 293/500 complete\n",
            "batch 294/500 complete\n",
            "batch 295/500 complete\n",
            "batch 296/500 complete\n",
            "batch 297/500 complete\n",
            "batch 298/500 complete\n",
            "batch 299/500 complete\n",
            "batch 300/500 complete\n",
            "batch 301/500 complete\n",
            "batch 302/500 complete\n",
            "batch 303/500 complete\n",
            "batch 304/500 complete\n",
            "batch 305/500 complete\n",
            "batch 306/500 complete\n",
            "batch 307/500 complete\n",
            "batch 308/500 complete\n",
            "batch 309/500 complete\n",
            "batch 310/500 complete\n",
            "batch 311/500 complete\n",
            "batch 312/500 complete\n",
            "batch 313/500 complete\n",
            "batch 314/500 complete\n",
            "batch 315/500 complete\n",
            "batch 316/500 complete\n",
            "batch 317/500 complete\n",
            "batch 318/500 complete\n",
            "batch 319/500 complete\n",
            "batch 320/500 complete\n",
            "batch 321/500 complete\n",
            "batch 322/500 complete\n",
            "batch 323/500 complete\n",
            "batch 324/500 complete\n",
            "batch 325/500 complete\n",
            "batch 326/500 complete\n",
            "batch 327/500 complete\n",
            "batch 328/500 complete\n",
            "batch 329/500 complete\n",
            "batch 330/500 complete\n",
            "batch 331/500 complete\n",
            "batch 332/500 complete\n",
            "batch 333/500 complete\n",
            "batch 334/500 complete\n",
            "batch 335/500 complete\n",
            "batch 336/500 complete\n",
            "batch 337/500 complete\n",
            "batch 338/500 complete\n",
            "batch 339/500 complete\n",
            "batch 340/500 complete\n",
            "batch 341/500 complete\n",
            "batch 342/500 complete\n",
            "batch 343/500 complete\n",
            "batch 344/500 complete\n",
            "batch 345/500 complete\n",
            "batch 346/500 complete\n",
            "batch 347/500 complete\n",
            "batch 348/500 complete\n",
            "batch 349/500 complete\n",
            "batch 350/500 complete\n",
            "batch 351/500 complete\n",
            "batch 352/500 complete\n",
            "batch 353/500 complete\n",
            "batch 354/500 complete\n",
            "batch 355/500 complete\n",
            "batch 356/500 complete\n",
            "batch 357/500 complete\n",
            "batch 358/500 complete\n",
            "batch 359/500 complete\n",
            "batch 360/500 complete\n",
            "batch 361/500 complete\n",
            "batch 362/500 complete\n",
            "batch 363/500 complete\n",
            "batch 364/500 complete\n",
            "batch 365/500 complete\n",
            "batch 366/500 complete\n",
            "batch 367/500 complete\n",
            "batch 368/500 complete\n",
            "batch 369/500 complete\n",
            "batch 370/500 complete\n",
            "batch 371/500 complete\n",
            "batch 372/500 complete\n",
            "batch 373/500 complete\n",
            "batch 374/500 complete\n",
            "batch 375/500 complete\n",
            "batch 376/500 complete\n",
            "batch 377/500 complete\n",
            "batch 378/500 complete\n",
            "batch 379/500 complete\n",
            "batch 380/500 complete\n",
            "batch 381/500 complete\n",
            "batch 382/500 complete\n",
            "batch 383/500 complete\n",
            "batch 384/500 complete\n",
            "batch 385/500 complete\n",
            "batch 386/500 complete\n",
            "batch 387/500 complete\n",
            "batch 388/500 complete\n",
            "batch 389/500 complete\n",
            "batch 390/500 complete\n",
            "batch 391/500 complete\n",
            "batch 392/500 complete\n",
            "batch 393/500 complete\n",
            "batch 394/500 complete\n",
            "batch 395/500 complete\n",
            "batch 396/500 complete\n",
            "batch 397/500 complete\n",
            "batch 398/500 complete\n",
            "batch 399/500 complete\n",
            "batch 400/500 complete\n",
            "batch 401/500 complete\n",
            "batch 402/500 complete\n",
            "batch 403/500 complete\n",
            "batch 404/500 complete\n",
            "batch 405/500 complete\n",
            "batch 406/500 complete\n",
            "batch 407/500 complete\n",
            "batch 408/500 complete\n",
            "batch 409/500 complete\n",
            "batch 410/500 complete\n",
            "batch 411/500 complete\n",
            "batch 412/500 complete\n",
            "batch 413/500 complete\n",
            "batch 414/500 complete\n",
            "batch 415/500 complete\n",
            "batch 416/500 complete\n",
            "batch 417/500 complete\n",
            "batch 418/500 complete\n",
            "batch 419/500 complete\n",
            "batch 420/500 complete\n",
            "batch 421/500 complete\n",
            "batch 422/500 complete\n",
            "batch 423/500 complete\n",
            "batch 424/500 complete\n",
            "batch 425/500 complete\n",
            "batch 426/500 complete\n",
            "batch 427/500 complete\n",
            "batch 428/500 complete\n",
            "batch 429/500 complete\n",
            "batch 430/500 complete\n",
            "batch 431/500 complete\n",
            "batch 432/500 complete\n",
            "batch 433/500 complete\n",
            "batch 434/500 complete\n",
            "batch 435/500 complete\n",
            "batch 436/500 complete\n",
            "batch 437/500 complete\n",
            "batch 438/500 complete\n",
            "batch 439/500 complete\n",
            "batch 440/500 complete\n",
            "batch 441/500 complete\n",
            "batch 442/500 complete\n",
            "batch 443/500 complete\n",
            "batch 444/500 complete\n",
            "batch 445/500 complete\n",
            "batch 446/500 complete\n",
            "batch 447/500 complete\n",
            "batch 448/500 complete\n",
            "batch 449/500 complete\n",
            "batch 450/500 complete\n",
            "batch 451/500 complete\n",
            "batch 452/500 complete\n",
            "batch 453/500 complete\n",
            "batch 454/500 complete\n",
            "batch 455/500 complete\n",
            "batch 456/500 complete\n",
            "batch 457/500 complete\n",
            "batch 458/500 complete\n",
            "batch 459/500 complete\n",
            "batch 460/500 complete\n",
            "batch 461/500 complete\n",
            "batch 462/500 complete\n",
            "batch 463/500 complete\n",
            "batch 464/500 complete\n",
            "batch 465/500 complete\n",
            "batch 466/500 complete\n",
            "batch 467/500 complete\n",
            "batch 468/500 complete\n",
            "batch 469/500 complete\n",
            "batch 470/500 complete\n",
            "batch 471/500 complete\n",
            "batch 472/500 complete\n",
            "batch 473/500 complete\n",
            "batch 474/500 complete\n",
            "batch 475/500 complete\n",
            "batch 476/500 complete\n",
            "batch 477/500 complete\n",
            "batch 478/500 complete\n",
            "batch 479/500 complete\n",
            "batch 480/500 complete\n",
            "batch 481/500 complete\n",
            "batch 482/500 complete\n",
            "batch 483/500 complete\n",
            "batch 484/500 complete\n",
            "batch 485/500 complete\n",
            "batch 486/500 complete\n",
            "batch 487/500 complete\n",
            "batch 488/500 complete\n",
            "batch 489/500 complete\n",
            "batch 490/500 complete\n",
            "batch 491/500 complete\n",
            "batch 492/500 complete\n",
            "batch 493/500 complete\n",
            "batch 494/500 complete\n",
            "batch 495/500 complete\n",
            "batch 496/500 complete\n",
            "batch 497/500 complete\n",
            "batch 498/500 complete\n",
            "batch 499/500 complete\n",
            "batch 500/500 complete\n",
            "Epoch 1/1 | Training Loss: 1.1277\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Really small datasets for demonstration purposes\n",
        "if len(train_df) > 1000:\n",
        "    train_df = train_df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
        "if len(test_df) > 1000:\n",
        "    test_df = test_df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "train_df = train_df.dropna(subset=[\"article\", \"highlights\"])\n",
        "test_df = test_df.dropna(subset=[\"article\", \"highlights\"])\n",
        "\n",
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, articles, highlights, tokenizer, max_input_len=512, max_target_len=128):\n",
        "        self.articles = articles\n",
        "        self.highlights = highlights\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_len = max_input_len\n",
        "        self.max_target_len = max_target_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.articles[idx]\n",
        "        summary = self.highlights[idx]\n",
        "\n",
        "        # Tokenize article\n",
        "        inputs = self.tokenizer(\n",
        "            article,\n",
        "            max_length=self.max_input_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        # Tokenize highlights\n",
        "        with self.tokenizer.as_target_tokenizer():\n",
        "            labels = self.tokenizer(\n",
        "                summary,\n",
        "                max_length=self.max_target_len,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': labels['input_ids'].squeeze()\n",
        "        }\n",
        "    \n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "print(\"model loaded...\")\n",
        "\n",
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "train_dataset = SummarizationDataset(\n",
        "    articles=train_df[\"article\"].tolist(),\n",
        "    highlights=train_df[\"highlights\"].tolist(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_input_len=max_input_length,\n",
        "    max_target_len=max_target_length\n",
        ")\n",
        "\n",
        "test_dataset = SummarizationDataset(\n",
        "    articles=test_df[\"article\"].tolist(),\n",
        "    highlights=test_df[\"highlights\"].tolist(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_input_len=max_input_length,\n",
        "    max_target_len=max_target_length\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "print(\"dataset loaded...\")\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "\n",
        "print(\"training started...\")\n",
        "\n",
        "# For demonstration, we will train for just 1 epoch\n",
        "# Note: In practice, you would want to train for more epochs (e.g., 3-5 or more)\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    num = 1\n",
        "    batch_count = len(train_loader)\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(\"batch {}/{} complete\".format(num, batch_count))\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num += 1\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Training Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Fine-tuning & Reporting BLEU/ROUGE\n",
        "\n",
        "We'll now evaluate on the **test set** and compute BLEU & ROUGE. For brevity, we only have a tiny test set of 1 example here, but the process is the same for larger test sets.\n",
        "\n",
        "### BLEU & ROUGE Setup\n",
        "- We can use `sacrebleu` for BLEU.\n",
        "- We can use `rouge_score` from `rouge-score` or Hugging Face's `evaluate.load(\"rouge\")`.\n",
        "\n",
        "In real usage, you'd likely average the scores across all test examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution_count": 3
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/colinfarley/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:1484: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (50). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score: 12.44\n",
            "ROUGE-1: 0.4047\n",
            "ROUGE-2: 0.1957\n",
            "ROUGE-L: 0.2930\n"
          ]
        }
      ],
      "source": [
        "import sacrebleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "model.eval()\n",
        "references = []\n",
        "hypotheses = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "    # Generate summary\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=50,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    for i, sid in enumerate(summary_ids):\n",
        "        # Decode reference and prediction\n",
        "        ref_text = tokenizer.decode(\n",
        "            batch['labels'][i], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        pred_text = tokenizer.decode(\n",
        "            sid, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        references.append(ref_text)\n",
        "        hypotheses.append(pred_text)\n",
        "\n",
        "# BLEU\n",
        "bleu_score = sacrebleu.corpus_bleu(hypotheses, [references])\n",
        "print(f\"BLEU score: {bleu_score.score:.2f}\")\n",
        "\n",
        "# ROUGE\n",
        "rouge_scorer_fn = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "r1, r2, rL = 0, 0, 0\n",
        "\n",
        "for ref, hyp in zip(references, hypotheses):\n",
        "    scores = rouge_scorer_fn.score(ref, hyp)\n",
        "    r1 += scores['rouge1'].fmeasure\n",
        "    r2 += scores['rouge2'].fmeasure\n",
        "    rL += scores['rougeL'].fmeasure\n",
        "\n",
        "n = len(references)\n",
        "print(f\"ROUGE-1: {r1/n:.4f}\")\n",
        "print(f\"ROUGE-2: {r2/n:.4f}\")\n",
        "print(f\"ROUGE-L: {rL/n:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of Hyperparameters & Model Choice\n",
        "\n",
        "1. **Learning Rate**:  \n",
        "   - We used `1e-5`, which often works well for fine-tuning large language models.  \n",
        "   - A larger LR (e.g., `1e-4`) might speed up convergence but increases the risk of instability and overfitting. \n",
        "\n",
        "2. **Max Sequence Length**:  \n",
        "   - BART supports up to 1024 tokens. Here, we used 512 for the article and 128 for the summary to reduce training time.  \n",
        "   - If your data contains very long articles, consider increasing these lengths (along with the associated memory/computational costs).\n",
        "\n",
        "3. **Batch Size**:  \n",
        "   - We used 2 for demonstration on CPU with a subset of data.  \n",
        "   - Larger batch sizes (e.g., 8 or 16) can expedite training and stabilize gradients when you have sufficient GPU memory.\n",
        "\n",
        "4. **Number of Beams**:  \n",
        "   - For generation, we set `num_beams=4`. Higher beam values (e.g., 8) may yield higher-quality summaries but slow down inference.  \n",
        "   - For quick experimentation or resource-constrained settings, a smaller beam might be best.\n",
        "\n",
        "---\n",
        "\n",
        "**Model Choice**:  \n",
        "- We opted for `facebook/bart-large-cnn`, which performs well on CNN/DailyMail summarization.  \n",
        "- Alternatives like `T5`, `Pegasus`, or even smaller DistilBART variants can be faster or yield different performance.  \n",
        "- Model performance depends on architecture, pre-training data, and hyperparameters.  \n",
        "\n",
        "**Summary of Current Results**:  \n",
        "- **BLEU**: 12.44  \n",
        "- **ROUGE-1**: 0.4047  \n",
        "- **ROUGE-2**: 0.1957  \n",
        "- **ROUGE-L**: 0.2930  \n",
        "\n",
        "These results reflect our chosen hyperparameters, smaller data subset, and a limited number of epochs. In practice, more extensive training, hyperparameter tuning, or using a different model variant could further improve the scores. Nonetheless, the above metrics demonstrate that our fine-tuned BART model captures core elements of the original text reasonably well, given the constraints.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Reinforcement Learning\n",
        "\n",
        "## Task 2 (20 points): Formulate a Real-World Application as an MDP\n",
        "\n",
        "### Example: Autonomous Driving at a Traffic Light\n",
        "\n",
        "We can formulate controlling an autonomous vehicle at intersections as an MDP.\n",
        "\n",
        "**State Space**:\n",
        "- The state could include the vehicle’s current speed, position, distance to intersection, time to traffic light change, positions/speeds of nearby cars, etc.\n",
        "\n",
        "**Action Space**:\n",
        "- The vehicle’s possible actions might be: `Accelerate`, `Maintain Speed`, `Decelerate`, or `Stop`.\n",
        "\n",
        "**Transition Model**:\n",
        "- If the vehicle chooses to accelerate, the new speed is increased by some amount, the position is updated accordingly, and the time to the next light change is decremented. The environment (traffic light, other vehicles) also evolves in response.\n",
        "- The transition probabilities could come from a traffic simulator or a real dataset of traffic patterns.\n",
        "\n",
        "**Reward**:\n",
        "- Primary goal: maximize safety and efficiency.\n",
        "- A negative reward for collisions or near-collisions.\n",
        "- A negative reward for running a red light.\n",
        "- Possibly a slight penalty for wasted time at red lights.\n",
        "- A positive reward for passing intersections safely and maintaining good speed.\n",
        "\n",
        "No explicit formula is required here, but qualitatively, the MDP structure is:\n",
        "- **States**: (vehicle state, environment state)\n",
        "- **Actions**: (acceleration / deceleration / etc.)\n",
        "- **Rewards**: (safety and efficiency metrics)\n",
        "- **Transition**: (updates from traffic model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3 (20 points): RL in Healthcare, Recommender Systems, or Trading\n",
        "\n",
        "### Pick One Area: Healthcare\n",
        "\n",
        "**Problem**: Optimizing Treatment Plans for Complex Medical Conditions  \n",
        "Healthcare often involves sequential decision-making under partial observability (e.g., evolving patient states that aren't fully known). Reinforcement Learning (RL) can help clinicians determine personalized treatment strategies by incorporating medical history, lab results, and other time-varying information. The goal is to learn policies that improve long-term patient outcomes (e.g., survival, reduced adverse events) rather than relying on static, rule-based approaches.\n",
        "\n",
        "---\n",
        "\n",
        "**Open-Source Project**: [An Empirical Study of Representation Learning for Reinforcement Learning in Healthcare](https://github.com/MLforHealth/rl_representations)  \n",
        "This repository explores how **representation learning** can improve RL in healthcare by modeling patient states more effectively under partial observability. It specifically focuses on a **sepsis** cohort from the MIMIC-III database, applying recurrent autoencoder variants to predict subsequent observations and learn informative patient-state embeddings. These learned representations are then used to train RL policies via Batch Constrained Q-learning (BCQ).\n",
        "\n",
        "---\n",
        "\n",
        "**Project Explanation**:\n",
        "- **Data & Environment**: The authors extract a sepsis cohort from the MIMIC-III database, refining code based on the original Sepsis cohort by Komorowski et al. (2018). They compute additional acuity scores (e.g., SOFA, SAPSII, OASIS) to enrich patient trajectories.\n",
        "- **Representation Learning**: Various sequential autoencoding architectures (e.g., RNN, ODERNN, AIS) learn to encode patient trajectories into a latent state that predicts future observations. This helps capture temporal and partial-observation dynamics in healthcare data.\n",
        "- **Policy Learning**: Once states are learned, a discrete BCQ approach (Fujimoto et al., 2019) is used to train and evaluate RL policies on the constructed state space. The code includes scripts for training policies, logging results, and performing weighted importance sampling–based evaluations.\n",
        "\n",
        "\n",
        "Using RL in this manner allows for dynamic policy updates in response to evolving patient states, leveraging learned representations that respect time-series dependencies and partial observability. This goes beyond simpler static rule-based healthcare interventions, potentially improving treatment safety and efficacy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Recommender Systems\n",
        "## Task 5 (30 points)\n",
        "\n",
        "We will use the **MovieLens 100k** dataset from [GroupLens](https://grouplens.org/datasets/movielens/100k/) to build and compare two collaborative filtering (CF) recommendation models.\n",
        "\n",
        "### Steps:\n",
        "1. Data Cleaning, EDA\n",
        "2. Convert to User-Item Matrix\n",
        "3. Implement **two** collaborative filtering methods (e.g., **SVD** and **NMF** from `Surprise` library, or ALS from some library like `implicit`).\n",
        "4. Compare performance using **two** evaluation metrics (e.g., **RMSE** and **MAE**, or Precision@k, Recall@k, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution_count": 4
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== NMF Results ===\n",
            "RMSE: 2.6232 | MAE: 2.3547\n",
            "\n",
            "=== TruncatedSVD Results ===\n",
            "RMSE: 632.7241 | MAE: 489.4774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/colinfarley/Library/Python/3.9/lib/python/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import NMF, TruncatedSVD\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import math\n",
        "\n",
        "data_path = \"data/ml-100k/u.data\"\n",
        "ratings_df = pd.read_csv(\n",
        "    data_path,\n",
        "    sep=\"\\t\",\n",
        "    names=[\"user\", \"item\", \"rating\", \"timestamp\"],\n",
        "    engine=\"python\"\n",
        ")\n",
        "\n",
        "ratings_df.drop_duplicates(inplace=True)\n",
        "ratings_df.dropna(inplace=True)\n",
        "\n",
        "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
        "\n",
        "all_users = pd.concat([train_df[\"user\"], test_df[\"user\"]]).unique()\n",
        "all_items = pd.concat([train_df[\"item\"], test_df[\"item\"]]).unique()\n",
        "\n",
        "user_to_idx = {u: i for i, u in enumerate(np.sort(all_users))}\n",
        "item_to_idx = {m: i for i, m in enumerate(np.sort(all_items))}\n",
        "\n",
        "num_users = len(user_to_idx)\n",
        "num_items = len(item_to_idx)\n",
        "\n",
        "train_matrix = np.zeros((num_users, num_items), dtype=np.float32)\n",
        "\n",
        "for row in train_df.itertuples():\n",
        "    u = user_to_idx[row.user]\n",
        "    i = item_to_idx[row.item]\n",
        "    train_matrix[u, i] = row.rating\n",
        "\n",
        "test_data = []\n",
        "for row in test_df.itertuples():\n",
        "    u = user_to_idx[row.user]\n",
        "    i = item_to_idx[row.item]\n",
        "    test_data.append((u, i, row.rating))\n",
        "\n",
        "n_components = 20\n",
        "nmf_model = NMF(n_components=n_components, init='random', random_state=42, max_iter=100)\n",
        "\n",
        "user_factors_nmf = nmf_model.fit_transform(train_matrix)\n",
        "item_factors_nmf = nmf_model.components_.T  # shape: (num_items, n_components)\n",
        "\n",
        "# Predict on test set\n",
        "predictions_nmf = []\n",
        "ground_truth = []\n",
        "for (u, i, true_rating) in test_data:\n",
        "    pred_rating = np.dot(user_factors_nmf[u], item_factors_nmf[i])\n",
        "    predictions_nmf.append(pred_rating)\n",
        "    ground_truth.append(true_rating)\n",
        "\n",
        "# Evaluate NMF\n",
        "mse_nmf = mean_squared_error(ground_truth, predictions_nmf)\n",
        "rmse_nmf = math.sqrt(mse_nmf)\n",
        "mae_nmf = mean_absolute_error(ground_truth, predictions_nmf)\n",
        "\n",
        "# We'll treat zero or missing entries as 0, which is not a perfect SVD scenario,\n",
        "# but it demonstrates a second matrix factorization approach.\n",
        "\n",
        "svd_model = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "user_factors_svd = svd_model.fit_transform(train_matrix)  # shape: (num_users, n_components)\n",
        "\n",
        "# Approx item factors by pseudo-inverse:\n",
        "Sigma = np.diag(svd_model.singular_values_)\n",
        "Vt = svd_model.components_  # shape: (n_components, num_items)\n",
        "\n",
        "item_factors_svd = (Sigma @ Vt).T  # shape: (num_items, n_components)\n",
        "\n",
        "# Now let's predict on test set\n",
        "predictions_svd = []\n",
        "for (u, i, true_rating) in test_data:\n",
        "    pred_rating = np.dot(user_factors_svd[u], item_factors_svd[i])\n",
        "    predictions_svd.append(pred_rating)\n",
        "\n",
        "# Evaluate TruncatedSVD\n",
        "mse_svd = mean_squared_error(ground_truth, predictions_svd)\n",
        "rmse_svd = math.sqrt(mse_svd)\n",
        "mae_svd = mean_absolute_error(ground_truth, predictions_svd)\n",
        "\n",
        "print(\"=== NMF Results ===\")\n",
        "print(f\"RMSE: {rmse_nmf:.4f} | MAE: {mae_nmf:.4f}\")\n",
        "\n",
        "print(\"\\n=== TruncatedSVD Results ===\")\n",
        "print(f\"RMSE: {rmse_svd:.4f} | MAE: {mae_svd:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f28be33",
      "metadata": {},
      "source": [
        "### Conclusion on NMF vs. TruncatedSVD\n",
        "\n",
        "The **NMF** approach yields a reasonable **RMSE** (~2.62) and **MAE** (~2.35), suggesting it fits the rating matrix relatively well. In contrast, **TruncatedSVD** shows very high error (RMSE ~632.72, MAE ~489.48), indicating poor reconstruction of user-item ratings under our current matrix setup.\n",
        "\n",
        "Several factors might explain why TruncatedSVD performs poorly here:\n",
        "\n",
        "1. **Zero-Filling**: We treat missing ratings as zeros, which can significantly skew an SVD-based method. Matrix factorization approaches like NMF often handle sparse data more gracefully if the missing entries are treated as truly unknown, while SVD sees them as zeros.\n",
        "\n",
        "2. **Implementation Detail**: With `TruncatedSVD`, we manually compute item embeddings via a pseudo-inverse operation. Minor differences or approximation errors can compound on top of zero-filling and lead to inflated predictions.\n",
        "\n",
        "3. **Model Constraints**: NMF imposes non-negative constraints on its factors, which can align better with rating data (generally non-negative). SVD does not have this constraint, potentially allowing negative factor entries or large factor norms that drive predictions to unrealistic magnitudes.\n",
        "\n",
        "#### Recommended Next Steps\n",
        "- **Adjust Data Handling**: Consider an approach that properly treats missing ratings rather than setting them to zero.  \n",
        "- **Hyperparameter Tuning**: For SVD or NMF, try varying the number of latent factors (`n_components`), regularization, or maximum iterations.  \n",
        "- **Alternative Methods**: Explore methods like ALS (Alternating Least Squares) or specialized recommendation frameworks.  \n",
        "- **Normalization**: Normalizing rows (users) or columns (items) might help SVD produce more stable factor embeddings.\n",
        "\n",
        "Overall, for this setup, **NMF** is more suitable for the given MovieLens dataset in terms of rating prediction accuracy.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "name": "Homework_6_Solution.ipynb"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
